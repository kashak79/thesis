\chapter{Framework: Pipes and Filters}
\label{newframework}

In this chapter, we design a framework based on the theoretical model from previous chapter. We start by establishing a small and general core architecture and continue with how we extended and used this core to build a flexible author-disambiguation tool. Implementation details of the framework and used algorithms and technologies in this framework will be discussed as well. Throughout the design, the requirements set in the foundation \autoref{foundation} and theoretical model were taken into account. At the end of the chapter we will discuss the strengths and weaknesses of the framework.

\section{Small and Simple Core}

We opted for a small and simple core framework. It is based on Pipes and Filters with a few extension. We chose to use this design pattern for the following reasons:

\begin{enumerate}
\item \textbf{Scalability} As said in Foundation \autoref{foundation:endless}, our algorithm needs to scale. Pipes and Filters makes this easy as all component are treated equally, can run independent from others and have a common interface.
\item \textbf{Modifiability} The Foundation (\autoref{foundation}) points to the highly dynamic environment several times. The framework will thus constantly be subject to new developments. These new developments will be in the form of new pipes, which can be plugged into the framework at any place.
\item \textbf{Maintainability} Using Pipes and Filters will reduce the complexity added by constantly modifying the framework. Pipes can always simply be replaced, moved and reconfigured in any way.
\end{enumerate}

\subsection{Architecture}

In figure \autoref{fig:architecturev2}, the architecture for the small Pipes and Filters core is shown. Each pipe has a number of input and output connectors. These connectors can be coupled to connectors of other pipes with a connection. When a pipe has executed it pushed its processed flow into the appropriate connector. This connector then pushes it to the connection that again pushes the flow into the connector of the next pipe. This pipe is then executed on the flow.

\begin{figure}[htp]
	\centering
		\includegraphics[width=0.8\textwidth]{fig/architecturev2}
	\caption{Pipes and Filters based Architecture.}
	\label{fig:architecturev2}
\end{figure}

The three classes Pipe, Connector and Connection are the real core of the architecture. All extensions and software to be created using this core will probably extend Pipe or Connection. Three important extensions are already present in the core architecture: LocalConnection, AsyncConnection and StatefulPipe. These classes will be discussed later on.

\subsection{Concepts, Terminology and Notations}

Together with this architecture we developed a few concepts that we use in combination with the core framework. These concepts allow us to build pipe configurations more easily and give us flexible control over the data flowing to the system. These concepts and associated terminology are explained in the following paragraphs.

\paragraph{Flows and Aspects} The information flowing through the pipes and connections are flows. Flows exist of different aspects. Every pipe is only interested in a few aspects of a flow. Other aspects can travel along too feed pipes later on the path. The choice of which aspect we process or send is completely free. An example of aspects are instance, publication or dependencies (see \autoref{dependencies}).

\paragraph{Connector Identifiers} Connectors need to be identified to allow us to connect a specific input or output to the right connection. The identifiers of the connectors can be any object. This makes configuring the pipes more expressive as identifiers suiting the context of the pipe can be used. For example, the output connector identifiers of the Filter pipe (\autoref{par:filterpipe}) are the booleans "true" and "false". We denote a input-connector with identifier $x$ as $Cn^i_x$ and an output-connector with identifier $y$ as $Cn^o_y$. The output ports of the Filter pipe would thus be denoted as $Cn^o_{true}$ and $Cn^o_{false}$.

\paragraph{Flow enrichment and filtering} In every pipe, we can push out whatever we want. However, a much used situation is where the pipe outputs the input flow with an extra aspect or a modified aspect. We call this flow enrichment. The other way around is also a possibility, any aspect or part of an aspect can be filtered in any pipe. However, this is not used in regular pipes but in a dedicated Filter pipe (\autoref{par:filterpipe}). Enriching and Filtering are denoted with plus and minus signs on the flow. An enriched flow A can be indicated with $A^+$ and a filtered flow A with $A^-$. The aspect that has been filtered or added can be indicated between brackets.

\paragraph{Flow Rate} Some pipes can increase or decrease the flow rate. A Filter for example will decrease the flow rate on both output connections. A Rule on the other hand will probably output multiple similarities for one input. Increasing the flow rate is denoted by adding a star (*) to the flow notation at the output. Decreasing the rate is indicated by a star on an input flow.

The notations we introduced are summarized in \autoref{fig:pipecomponents}.

\begin{figure}[htb]
	\centering
		\includegraphics[width=0.75\textwidth]{fig/pipecomponents}
	\caption{Pipe components, terminology and notations.}
	\label{fig:pipecomponents}
\end{figure}

\subsection{Core Extensions}

In the architecture (\autoref{fig:architecturev2}), we already mentioned three core extensions. These extensions extensively used throughout the program, so we included them in the architecture. Each of these three extensions is explained below.

\paragraph{Local Connections}

The most basic connection between two local pipes is one that just pushes the flow forward without memorizing anything. This connection is the bread and butter of configuring pipe networks in our program.

\paragraph{Asynchronous Connections}

A more complex connection is the asynchronous connection. It allows us to run any pipe in a distributed way. When a connection is made asynchronous, the flow of the pipe will be interpreted as the description of a job. This job will be pushed to a queue in a distributed shared memory store. A predefined set of workers ran on different machines will poll this queue constantly and execute the jobs on the machine they are running. This way, the load pushed to the pipes can be evenly distributed over a set of nodes in a computing cluster. The technology we use for this and extra details can be found in \autoref{resque}.

\paragraph{Stateful Pipes}

For some pipes, it is required to have a non-transient memory, a memory that survives between executions. This allows the pipe to process input differently based on what is present in its long-time memory. As Pipes can be run in a distributed fashion, it is possible for a pipe to be initialized on another physical machine for every execution. If the memory would be local, bound to a machine, the pipe would lose access to it if it was executed on another machine. To solve this, we use a distributed shared memory store (\autoref{redis}).

\subsection{Implementation details and Technologies}

\paragraph{The Use of Ruby and Java}

We implemented the entire framework in Ruby. We chose ruby because of the following reasons:

\begin{enumerate}
\item It is a concise and powerful language without many configuration which allows us to build something useful relatively fast.
\item It has a great community, we benefit from this as we make use of several excellent software packages from the open source ruby community.
\end{enumerate}

Pipes can either be written in Ruby or Java. We have used Ruby for almost all of them, but the clustering Pipe uses Java. This is because we use a library (JUNG) to calculate the maximum flow in a graph.

<<<<<<< HEAD:boek/newframework.tex
\paragraph{A Distrbuted Shared Key-Value Store: Redis}
\label{redis}
=======
\paragraph{A Distributed Shared Key-Value Store: Redis}
>>>>>>> ada8a6047328b60dd37ec99e92660f9205588a3c:boek/newframework.tex

We chose Redis to serve as our distributed key-value store. It provides the functionality we need and has a great client for Ruby. It is also used by Resque (explained in the next paragraph), so choosing this as our distributed store prevents us from having to install two different solutions.

Redis also has a much larger set of commands than just the get and set primitives. It has built-in atomic actions for almost anything. It can also act as a tool to provide synchronization. This is necessary as we plan to run our pipe configuration over multiple machines. It allows us to use bots pessimistic and optimistic locking.

\paragraph{A Distributed Job Processing Framework: Resque}
\label{resque}

Resque is a Redis-backed library for creating background jobs, placing those jobs on multiple queues, and processing them later. Resque is used at Github to process millions of jobs each day. It spawns a pool of workers that poll the queues for jobs. These jobs are then executed in separate processes so that they can be terminated if things get out of hand (ex. memory leaks). It is very easy to set up and provides us with a Web-based user interface to inspect our jobs and workers.

\section{Generally Useful Pipes}

With these small and simple Pipes and Filters core we built some useful pipes along the way. Clearly the filter pipe is important to have in a Pipes and Filters architecture, so we explain this one first. Merging and Splitting flows, both extensively used pipes, are mentioned second. Thirdly we introduce a pipe that allows us to do network operations. Last, we discuss two pipes more specific for our case but as they are used throughout the entire pipe system, we mention them here.

\paragraph{Filter} \label{par:filterpipe} The Filter pipe allows us to filter flow in two ways, as you can also see on \autoref{fig:filter}. First, it is possible to test a condition for each arriving flow. Depending on this condition, the flow is forwarded to either the connector with identifier "true" or identifier "false". On top of that we are able to filter aspects of the flow itself. This Filter pipe gives us the possibility to control the flow in our pipe configuration. The flows should be small and contain only aspects that are necessary.

\begin{figure}[htp]
	\centering
		\includegraphics[width=0.75\textwidth]{fig/filter}
	\caption{Filter pipe.}
	\label{fig:filter}
\end{figure}

\paragraph{Merge and Split} In the case we want to forward the result from one pipe to multiple pipes or from multiple pipes to one, we need a Split or Merge pipe. Split pipes have the default ":in" connector and split their input over $n$ output connectors identified by $1,\ldots,n$. A Merge pipe does the inverse by merging input connections $1,\ldots,n$ into one connector identified by the default ":out".

\begin{figure}[htb]
	\centering
		\includegraphics[width=0.75\textwidth]{fig/mergeandsplit}
	\caption{Merge and Split pipes.}
	\label{fig:mergeandsplit}
\end{figure}

\paragraph{Network} The network pipe has exactly one input and output. It expects an inflow of url's and provides an outflow of the contents of the web-pages found at these url's.

\section{Working Towards a Contextual System}

We now apply these general concepts and designs in the context of our thesis problem. First we agree upon a set of flow types. A flow type marks a flow and is used to distinguish flow from each other. In short, the path of a flow through a pipe configuration will depend on its type.

\subsection{Flow Types} We divide flow into four types. Two of these types carry crawled information from the web. The flow associated with these types are "information flows. The other two accompany "control messages". We indicate the type of flow $A$ as $A_type$.

\begin{enumerate}
\item \textbf{Discovery}: The discovery of a new piece of information. The source can be anything but will be probably the web or a publication document. The most important discoveries are instances and publications.
\item \textbf{Fact}: The finding of a relation between two entities is called a fact. The most important flow with this type the one with the fact that a certain instance published a publication (published fact).
\item \textbf{Similarity} Rules (\autoref{rules}) process these two types and generate similarities. These similarities are again sent through as a flow message.
\item \textbf{Cluster} Similarity messages are intended for clustering pipes, these pipes interpret the similarities and make the appropriate changes in the author clusters.
\end{enumerate}

After establishing these four main message types, we created another two generally usable pipes. Persisting discoveries is done with the Persist Discovery pipe. It provides automatic persisting of sets, automatic indexing and avoidance of duplicate discoveries. The Persist Fact pipe takes care of the same tasks for facts.

\subsection{A Graph Database: Neo4j in Combination With the Tinkerpop Stack}

As already showed, Neo4j performs much better when it comes to querying with a large depth. This is the same as doing joins in MySQL which is slow. That is why we chose Neo4j to serve as our database. On top of Neo4j we use the Tinkerpop stack. This stack creates an abstract layer above several graph databases. It uses a general property graph model to persist and query data in the graph.

We do not use SPARQL to issue our queries. We use Gremlin, part of the Tinkerpop stack. Gremlin also allows us to do pattern matching on the graph but provides us with much more flexibility in terms of controlling what is done while Gremlin traverses the graph.

Communicating with Tinkerpop is done through a HTTP interface, causing a lot of I/O waiting, but as we distribute our work to different workers, the impact of this waiting decreases.

\section{Pipe Configuration}

In this section we will give an overview of the entire pipe configuration of our program. We have divided this configuration into parts, each with their own responsibilities, difficulties and used technologies. We first discuss the information retrieval using parsers. Then we discuss the parts that form the structural layer, followed by the informational layer and the similarity layer.

\subsection{Parsers}

Retrieving information from external sources is a task of the parsers. We currently only use DBLP as an external source. Each of the author pages we want to process is fed to the parser. The outflow exists out of publication discoveries, instances discoveries and "published"-facts. We achieve this by first creating a pipe that searches the contents of the author page for links to XML representations of each of the publications. Each of this links then flows through a Network pipe, again collecting its contents. The according XML representation is then parsed and the extracted flows are pushed.

\subsection{Instance Integration}

When a new instance is discovered, it has to be integrated in the graph. Later on, information can be attached to this instance to feed the rules. The different steps of this integration have been explained in \autoref{structurallayer}. An overview of the pipe configuration is found in \autoref{fig:integrationpipe}.

First, the incoming instance discovery is lead to the "Persist Instance" pipe. This pipe persists the instance in the graph. As every new instance is assigned its very own author (cluster), the cluster is also persisted and the appropriate edges are added. The subsequent two pipes on the path deal with the persisting the family and the name nodes. If a name is already present in the system, the integration of the instance has finished. Otherwise, we must execute a name matching algorithm. This algorithm is explained in the next paragraph.

\begin{figure}[htb]
	\centering
		\includegraphics[width=0.75\textwidth]{fig/integrationpipe}
	\caption{Pipe Configuration for Instance Integration.}
	\label{fig:integrationpipe}
\end{figure}

\paragraph{Name Matching} Name Matching is the process of matching the name of a family with the names already present in the family. The goal is to reduce the problem domain size (\autoref{problemdomain}). When a new name is added, we query all the names in the family and compare them to the name that is being added. We do this using a fuzzy string matching algorithm based on Dynamic Time Warping. Taking properties of names into account lead to some modifications of the algorithm, making it especially suitable for name matching.

\begin{enumerate}
\item We use a token based approach (tokens between whitespace). The tokens themselves are compared using the Jaro Winkler fuzzy string matching algorithm.
\item The algorithm is always started from the viewpoint of the name with the least amount of tokens. An insertion has a cost of zero and a deletion is heavily penalized.
\item In case of a short token (one symbol or one symbol followed by a dot), we do not use a fuzzy approach. If the first letter matches with the first letter of the token it is being compared with, we assign a minimum cost, otherwise we assign a maximum cost.
\end{enumerate}

\subsection{Magic Facts}

As we also make use of email and affiliation rules, we needed to find a way to collect this data. We first attempted to retrieve publication documents via Microsoft Bing. Google scholar is not usable due to its security against programmatic access. The number of publications found by Bing was not enough to feed the rules. Although we created a pipe that can extract email addresses out of PDF documents and assign them to the correct author, we did not use it. We did this partly because information retrieval is not our main focus and not the real challenge. It is definitely possible to collect all this data with more effort. However, we thought it was more important to research the power of this information in the context of disambiguation.

To still be able to use the data, we manually collected email addresses and affiliations out of the publications of the authors in our test set. As this information makes a magical appearance in our system, we called the pipe that provides it "Magic Facts". Magic Facts takes a published fact, giving us the publication and instance aspects, as its input and enriches it with the email and affiliation of the instance for that publication. The pipe is illustrated in \autoref{fig:magicfacts}.

\begin{figure}[htb]
	\centering
		\includegraphics[width=0.75\textwidth]{fig/magicfactspip}
	\caption{Pipe Configuration for Magic Facts.}
	\label{fig:magicfacts}
\end{figure}

\subsection{Publications}

As a part of the informational layer (\autoref{informationallayer}), we need to persist publications in the graph along with extracted keywords. The configuration of this pipe is rather simple, it is shown in \autoref{fig:publicationpipe}. As you can see, we make good use of the PersistDiscovery and PersistFact pipes. The keyword extractor extracts keywords that define the domain of the publication. How we manage to extract this keywords is explained in the following paragraph.

\begin{figure}[htb]
	\centering
		\includegraphics[width=0.75\textwidth]{fig/publicationpipe}
	\caption{Pipe Configuration for Publication Processing.}
	\label{fig:publicationpipe}
\end{figure}

\paragraph{Keyword Extraction} To extract the keywords out of the title of a publication we use a Part of Speech Tagger. This tagger is able to identify the types of the words present in the title. Using Engtagger, a simple PoS tagger for Ruby, we extract nouns and noun phrases. The more nouns in a noun phrase, the more specific this noun phrase is. This property is being exploited in the email rule (\autoref{emailrule}).

\subsection{Rules}

Rules are the essence of the framework, it is what fulfills the goal of this framework: disambiguating authors. All the previous steps in building the framework were in order to manage the execution of these rules and getting the right information to them. A Rule basically inspects the incoming flow to initialize its context en subsequently issues a query on the graph to derive similarities. We refer to \autoref{rules} for an elaborate investigation on the working of these rules.

Most of the queries for these rules are not that difficult to implement with the help of Gremlin. However, the community rule variants require more complex queries. We explain the variant where we deal with an exact name match on the one side and a similar name on the other side. 

We start in the instance V. We need to query all instances that share a name with V. This is done by following "name" to the name node and following it back out so all instances with this name are reached (except node V). The names found here are labeled W. To find the co-instances of each of these nodes we follow the edges labeled "published". These instances are labeled Y. Now, we again follow the "name" labeled edges but this time we follow "matches" in between to get all names that are similar with the names of Y. The instances of these names are labeled X. X is now an large collection of names, but most of them do not satisfy the last condition: Any instance in X must be a co-instance of V. We require this by adding a retain pipe. The id's of the four sets of instances (V,W,Y,X) are returned in a table. Now we can add the appropriate similarities between the instances. The implementation of this query is shown below.

\begin{verbatim}
v.as("V")
  .out("name").in("name")
  .except(v).as("W")
  .out("published").in("published").as("Y")
  .out("name").both("matches").in("name").as("X")
  .retain(v.out("published").in("published").except(v).to_a)
  .table(:id,:id,:id,:id)
\end{verbatim}

All similarities produces by these rules are merged into one stream which is fed to the clustering pipe. This is illustrated in \autoref{fig:clusteringpipe}. An overview of the pipe configuration of the several rules is shown in \autoref{fig:rulespipe}.

\begin{figure}[htb]
	\centering
		\includegraphics[width=1\textwidth]{fig/clusteringpipe}
	\caption{Clustering flow}
	\label{fig:clusteringpipe}
\end{figure}

\begin{figure}[htb]
	\centering
		\includegraphics[width=1\textwidth]{fig/rulespipe}
	\caption{Pipe Configuration of the Rules Network}
	\label{fig:rulespipe}
\end{figure}

\subsection{Concurrent, incremental Clustering}

Clustering is a very important step in building towards a solution. Each new flow of information indirectly leads to a clustering operation. Acquiring new information triggers rules which on their turn yield similarities. These similarities change the balance between clusters. In the worst case, an expensive rebalancing procedure is necessary.

<<<<<<< HEAD:boek/newframework.tex
It is obvious that processing similarities is something that will be executed very frequently (numbers?). The combination of the enormous amount of similarities and their expensive processing requires us to make this process as streamlined and efficient as possible. The clustering algorithm as explained in \autoref{clustering} leads to a first, naive implementation approach. Rethinking the absolute needs of the clustering algorithm then leads to a second approach that benifits greatly of the foundations of our framework.
=======
It is obvious that processing similarities is something that will be executed very frequently (numbers?). The combination of the enormous amount of similarities and their expensive processing requires us to make this process as streamlined and efficient as possible. The clustering algorithm as explained in \autoref{clustering} leads to a first, naive implementation approach. Rethinking the absolute needs of the clustering algorithm then leads to a second approach that benefits greatly of the foundations of our framework.
>>>>>>> ada8a6047328b60dd37ec99e92660f9205588a3c:boek/newframework.tex

\paragraph{In-graph implementation} The most simple solution one can think of is to maintain the ICW and OCW for every vertex in the vertex itself. The adjacency matrix would implicitly be defined by the edges between two nodes in the similarity plane. This technique has two main drawbacks:

\begin{enumerate}
\item A lot of load would be pushed to the database.
\item There would be a need for several concurrency control mechanisms/
\end{enumerate}

If clustering could be executed without the use of the database in an efficient manner, it would be preferable. After all it is in our best interest to take as much load as possible away from the database because it is much more difficult to scale than our pipes and filters architecture. Besides, the similarity plane is not something that should be queried from our end-user application. The users are interested in the result of the clustering, not the way we got there.

\paragraph{As a stateful pipe}

Currently, the clustering algorithm is implemented as a stateful pipe. This means that all values of the adjacency matrix, the OCW's and ICW's  are maintained in our distributed key-value store. The id's of both involved clusters are provided and the graph is not needed anymore. This way, clustering is just a pipe like any other and therefore completely distributable.

In the case of similarities being processed in parallel, we need to pay some extra attention. We do not want that the clustering mechanism inflicts race conditions and, by consequence, inconsistencies on the graph. In case of an intra-cluster similarity being added, we actually do not need locks as we can make use of a technique call optimistic locking. This technique assumes that multiple transactions can complete without affecting each other, and that therefore transactions can proceed without locking the data resources that they affect. Before committing, each transaction verifies that no other transaction has modified its data. Intra-cluster similarities can always be processed in parallel for any two instances. Inter-cluster similarities on the other hand require that no changes are made to the entire cluster.

In case of an intra-cluster addition, we could watch the locks of both involved instances for modifications. If they are not set and do not change during the processing of the similarity, the transaction (intra\_add) succeeds. If the locks were modified the transaction will fail and we will retry to process the similarities. In case of an inter-cluster addition, the entire two clusters will be locked. This flow is shown in Algorithm \autoref{locking}. The cluster method locks all the instances of the clusters  of the provided instances in one transaction.

\begin{algorithm}
\caption{Locking mechanism to control concurrent similarity processing.}
\label{locking}
\begin{algorithmic}
\IF{$cluster(I_1) == cluster(I_2)$}
  \STATE \textbf{label} retry
  \STATE \textbf{watch} ${lock}_{I_1},{lock}_{I_2}$
  \IF{${lock}_{I_1}$ and ${lock}_{I_2}$}
  \STATE \textbf{goto} retry
  \ELSIF{!transaction(intra\_add$(I_1,I_2))$}
    \STATE \textbf{goto} retry
  \ENDIF
\ELSE
  \STATE \textbf{lock} cluster($I_1$),cluster($I_2$)
  \STATE inter\_add$(I_1,I_2))$
\ENDIF
\end{algorithmic}
\end{algorithm}

\subsection{Dependency Resolution}
\label{dependencies}

Another difficulty turning up in our framework is the dependency of certain flow onto others. In this case, a flow can not be executed untill its dependencies are resolved. An example is the published fact: we are not able to process this until the associated instance is integrated in the graph. To solve this we designed a stateful pipe that manages these dependencies. A dependent flow then waits in this pipe until all its dependencies are resolved.

\subsection{Overview}

The previous sections each described a component of our pipe network. All of this components and pipes need to word together. How this is done is illustrated in \autoref{completepipesmall}. We added a Message Distribution pipe with the responsibility of forwarding the messages with the right types to right pipes.

\begin{figure}[htb]
	\centering
		\includegraphics[width=1\textwidth]{fig/completepipesmall}
	\caption{Clustering flow}
	\label{fig:completepipesmall}
\end{figure}


