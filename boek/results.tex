\chapter{Framework evaluation and results}

We need to evaluate the performance of the framework. We will mainly focus our tests on the clustering of authors and the influence of different combinations of rules and parameters on the results. Proper clustering results in a collection of publications related to one author, allowing to define specialties and the level of expertise. 

In order to accomplish realistic and useful results, it's important the tests resemble realistic use-cases. This means testing against both general and borderline queries. Parameters making it easier in disambiguation are rare and country specific author names, a multitude of publications which are written with the same co-authors or the inclusion of the same email address. A combination of these parameters allows for easier manual checking of results, but is consequently less challenging. 

Borderline cases make more defiant evalutations. Author names containing foreign characters or accents make it easier to be misspelled. In contrast, common last names also make it harder to differentiate between different authors. Examples are Anderson or Smith in United States, Chen or Lee in East Asian countries or Peters in Belgium.

% Wat moeten we bespreken in dit hoofdstuk:

% De testopstelling
	% Hoe zit deze in elkaar
	% Waarom hebben hiervoor gekozen
	% Vergelijking met anderen?
% De bekomen resultaten
% (Vergelijken met andere mensen)

% Moeten we dit doen voor de clustering (dus vooral kijken naar name disambiguation) en de expert finding (hiervoor hebben we voornamelijk de category builder nodig voor goede resultaten)

\section{Evaluation setup}

\cite{han2004two} focuses on disambiguating between authors having the same name or names which are very closely related. They have one dataset consisting of fifteen different authors having the name "`J. Anderson"' and a training set containing 117 publications which they have manually collected from the web, mainly from publication lists from homepages. A second dataset consists of nine other names, with publications collected from DBLP. An overview of this dataset can been viewed on \autoref{table:auth-dblp-dataset}. The mean accuracy they accomplish with their best approach is $73.3\%$ with a standard deviation of $5.4\%$.

\begin{table}
	\centering
		\begin{tabular}[ht]{|c|c|c|}
			\hline
			\bfseries{Name Set} & \bfseries{Name Variations} & \bfseries{Training Size} \\
			\hline
			S Lee & 35 & 244 \\
			\hline
			J Lee & 33 & 172 \\
			\hline
			J Kim & 25 & 127 \\
			\hline
			Y Chen & 24 & 108 \\
			\hline
			S Kim & 20 & 94 \\
			\hline
			C Lee & 18 & 80 \\
			\hline
			A Gupta & 16 & 172 \\
			\hline
			J Chen & 13 & 91 \\
			\hline
			H Kim & 11 & 63 \\
			\hline
		\end{tabular}:p
	\caption{The nine DBLP datasets. Each row contains the base name, the number of recorded variations and the total amount of examined publications of this name.}
	\label{tab:TheNineDBLPDatasetsOfDifferentNamesAndTheDataSize}
\end{table}

We have mailed the authors of the paper \cite{han2004two} and have received the DBLP datasets, already ordered by cluster. However, as the datasets are this huge, manual checking is very labor-intensive. The clusters are solely based on the division of the authors in DBLP. The quality of the disambiguation from DBLP, however, sometimes falls short. As we would still have to check the results manually to gain a proper insight into the disambiguation quality, we choose to compose our own dataset.

